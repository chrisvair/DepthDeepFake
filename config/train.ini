[global]
# OPTIONS THAT APPLY TO ALL MODELS
# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.

# How to center the training image. The extracted images are centered on the middle of the skull based
# on the face's estimated pose. A subsection of these images are used for training. The centering used
# dictates how this subsection will be cropped from the aligned images.
#     - face: Centers the training image on the center of the face, adjusting for pitch and yaw.
#     - head: Centers the training image on the center of the head, adjusting for pitch and yaw. NB:
# 		You should only select head centering if you intend to include the full head (including hair) in
# 		the final swap. This may give mixed results. Additionally, it is only worth choosing head
# 		centering if you are training with a mask that includes the hair (e.g. BiSeNet-FP-Head).
#     - legacy: The 'original' extraction technique. Centers the training image near the tip of the
# 		nose with no adjustment. Can result in the edges of the face appearing outside of the training
# 		area.
# 
# Choose from: ['face', 'head', 'legacy']
# [Default: face]
centering = face

# How much of the extracted image to train on. A lower coverage will limit the model's scope to a
# zoomed-in central area while higher amounts can include the entire face. A trade-off exists between
# lower amounts given more detail versus higher amounts avoiding noticeable swap transitions. For
# 'Face' centering you will want to leave this above 75%. For Head centering you will most likely want
# to set this to 100%. Sensible values for 'Legacy' centering are:
#     - 62.5% spans from eyebrow to eyebrow.
#     - 75.0% spans from temple to temple.
#     - 87.5% spans from ear to ear.
#     - 100.0% is a mugshot.
# 
# Select a decimal number between 62.5 and 100.0
# [Default: 87.5]
coverage = 87.5

# Use ICNR to tile the default initializer in a repeating pattern. This strategy is designed for
# pairing with sub-pixel / pixel shuffler to reduce the 'checkerboard effect' in image reconstruction.
#     - https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf
# 
# Choose from: True, False
# [Default: False]
icnr_init = False

# Use Convolution Aware Initialization for convolutional layers. This can help eradicate the vanishing
# and exploding gradient problem as well as lead to higher accuracy, lower loss and faster
# convergence.
# NB:
#     - This can use more VRAM when creating a new model so you may want to lower the batch size for
# 		the first run. The batch size can be raised again when reloading the model.
#     - Multi-GPU is not supported for this option, so you should start the model on a single GPU.
# 		Once training has started, you can stop training, enable multi-GPU and resume.
#     - Building the model will likely take several minutes as the calculations for this
# 		initialization technique are expensive. This will only impact starting a new model.
# 
# Choose from: True, False
# [Default: False]
conv_aware_init = False

# The optimizer to use.
#     - adabelief - Adapting Stepsizes by the Belief in Observed Gradients. An optimizer with the aim
# 		to converge faster, generalize better and remain more stable. (https://arxiv.org/abs/2010.07468).
# 		NB: Epsilon for AdaBelief needs to be set to a smaller value than other Optimizers. Generally
# 		setting the 'Epsilon Exponent' to around '-16' should work.
#     - adam - Adaptive Moment Optimization. A stochastic gradient descent method that is based on
# 		adaptive estimation of first-order and second-order moments.
#     - nadam - Adaptive Moment Optimization with Nesterov Momentum. Much like Adam but uses a
# 		different formula for calculating momentum.
#     - rms-prop - Root Mean Square Propagation. Maintains a moving (discounted) average of the square
# 		of the gradients. Divides the gradient by the root of this average.
# 
# Choose from: ['adabelief', 'adam', 'nadam', 'rms-prop']
# [Default: adam]
optimizer = adam

# Learning rate - how fast your network will learn (how large are the modifications to the model
# weights after one batch of training). Values that are too large might result in model crashes and
# the inability of the model to find the best solution. Values that are too small might be unable to
# escape from dead-ends and find the best global minimum.
# 
# This option can be updated for existing models.
# 
# Select a decimal number between 1e-06 and 0.0001
# [Default: 5e-05]
learning_rate = 5e-05

# The epsilon adds a small constant to weight updates to attempt to avoid 'divide by zero' errors.
# Unless you are using the AdaBelief Optimizer, then Generally this option should be left at default
# value, For AdaBelief, setting this to around '-16' should work.
# In all instances if you are getting 'NaN' loss values, and have been unable to resolve the issue any
# other way (for example, increasing batch size, or lowering learning rate), then raising the epsilon
# can lead to a more stable model. It may, however, come at the cost of slower training and a less
# accurate final result.
# NB: The value given here is the 'exponent' to the epsilon. For example, choosing '-7' will set the
# epsilon to 1e-7. Choosing '-3' will set the epsilon to 0.001 (1e-3).
# 
# This option can be updated for existing models.
# 
# Select an integer between -20 and 0
# [Default: -7]
epsilon_exponent = -7

# When to save the Optimizer Weights. Saving the optimizer weights is not necessary and will increase
# the model file size 3x (and by extension the amount of time it takes to save the model). However, it
# can be useful to save these weights if you want to guarantee that a resumed model carries off
# exactly from where it left off, rather than spending a few hundred iterations catching up.
#     - never - Don't save optimizer weights.
#     - always - Save the optimizer weights at every save iteration. Model saving will take longer,
# 		due to the increased file size, but you will always have the last saved optimizer state in your
# 		model file.
#     - exit - Only save the optimizer weights when explicitly terminating a model. This can be when
# 		the model is actively stopped or when the target iterations are met. Note: If the training session
# 		ends because of another reason (e.g. power outage, Out of Memory Error, NaN detected) then the
# 		optimizer weights will NOT be saved.
# 
# This option can be updated for existing models.
# 
# Choose from: ['never', 'always', 'exit']
# [Default: exit]
save_optimizer = exit

# The number of iterations to process to find the optimal learning rate. Higher values will take
# longer, but will be more accurate.
# 
# Select an integer between 100 and 10000
# [Default: 1000]
lr_finder_iterations = 1000

# The operation mode for the learning rate finder. Only applicable to new models. For existing models
# this will always default to 'set'.
#     - set - Train with the discovered optimal learning rate.
#     - graph_and_set - Output a graph in the training folder showing the discovered learning rates
# 		and train with the optimal learning rate.
#     - graph_and_exit - Output a graph in the training folder with the discovered learning rates and
# 		exit.
# 
# Choose from: ['set', 'graph_and_set', 'graph_and_exit']
# [Default: set]
lr_finder_mode = set

# How aggressively to set the Learning Rate. More aggressive can learn faster, but is more likely to
# lead to exploding gradients.
#     - default - The default optimal learning rate. A safe choice for nearly all use cases.
#     - aggressive - Set's a higher learning rate than the default. May learn faster but with a higher
# 		chance of exploding gradients.
#     - extreme - The highest optimal learning rate. A much higher risk of exploding gradients.
# 
# Choose from: ['default', 'aggressive', 'extreme']
# [Default: default]
lr_finder_strength = default

# Apply AutoClipping to the gradients. AutoClip analyzes the gradient weights and adjusts the
# normalization value dynamically to fit the data. Can help prevent NaNs and improve model
# optimization at the expense of VRAM. Ref: AutoClip: Adaptive Gradient Clipping for Source Separation
# Networks https://arxiv.org/abs/2007.14469
# 
# This option can be updated for existing models.
# 
# Choose from: True, False
# [Default: False]
autoclip = False

# Use reflection padding rather than zero padding with convolutions. Each convolution must pad the
# image boundaries to maintain the proper sizing. More complex padding schemes can reduce artifacts at
# the border of the image.
#     - http://www-cs.engr.ccny.cuny.edu/~wolberg/cs470/hw/hw2_pad.txt
# 
# Choose from: True, False
# [Default: False]
reflect_padding = False

# Enable the Tensorflow GPU 'allow_growth' configuration option. This option prevents Tensorflow from
# allocating all of the GPU VRAM at launch but can lead to higher VRAM fragmentation and slower
# performance. Should only be enabled if you are receiving errors regarding 'cuDNN fails to
# initialize' when commencing training.
# 
# This option can be updated for existing models.
# 
# Choose from: True, False
# [Default: False]
allow_growth = False

# NVIDIA GPUs can run operations in float16 faster than in float32. Mixed precision allows you to use
# a mix of float16 with float32, to get the performance benefits from float16 and the numeric
# stability benefits from float32.
# 
# This is untested on DirectML backend, but will run on most Nvidia models. it will only speed up
# training on more recent GPUs. Those with compute capability 7.0 or higher will see the greatest
# performance benefit from mixed precision because they have Tensor Cores. Older GPUs offer no math
# performance benefit for using mixed precision, however memory and bandwidth savings can enable some
# speedups. Generally RTX GPUs and later will offer the most benefit.
# 
# This option can be updated for existing models.
# 
# Choose from: True, False
# [Default: False]
mixed_precision = False

# If a 'NaN' is generated in the model, this means that the model has corrupted and the model is
# likely to start deteriorating from this point on. Enabling NaN protection will stop training
# immediately in the event of a NaN. The last save will not contain the NaN, so you may still be able
# to rescue your model.
# 
# This option can be updated for existing models.
# 
# Choose from: True, False
# [Default: True]
nan_protection = True

# [GPU Only]. The number of faces to feed through the model at once when running the Convert process.
# 
# NB: Increasing this figure is unlikely to improve convert speed, however, if you are getting Out of
# Memory errors, then you may want to reduce the batch size.
# 
# This option can be updated for existing models.
# 
# Select an integer between 1 and 32
# [Default: 16]
convert_batchsize = 16

[global.loss]
# LOSS CONFIGURATION OPTIONS
# LOSS IS THE MECHANISM BY WHICH A NEURAL NETWORK JUDGES HOW WELL IT THINKS THAT IT IS RECREATING A
# FACE.
# NB: UNLESS SPECIFICALLY STATED, VALUES CHANGED HERE WILL ONLY TAKE EFFECT WHEN CREATING A NEW MODEL.

# The loss function to use.
# 
#     - ffl: Focal Frequency Loss. Analyzes the frequency spectrum of the images rather than the
# 		images themselves. This loss function can be used on its own, but the original paper found
# 		increased benefits when using it as a complementary loss to another spacial loss function (e.g.
# 		MSE). Ref: Focal Frequency Loss for Image Reconstruction and Synthesis
# 		https://arxiv.org/pdf/2012.12821.pdf NB: This loss does not currently work on AMD cards.
# 
#     - gmsd: Gradient Magnitude Similarity Deviation seeks to match the global standard deviation of
# 		the pixel to pixel differences between two images. Similar in approach to SSIM. Ref: Gradient
# 		Magnitude Similarity Deviation: An Highly Efficient Perceptual Image Quality Index
# 		https://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf
# 
#     - l_inf_norm: The L_inf norm will reduce the largest individual pixel error in an image. As each
# 		largest error is minimized sequentially, the overall error is improved. This loss will be
# 		extremely focused on outliers.
# 
#     - laploss: Laplacian Pyramid Loss. Attempts to improve results by focussing on edges using
# 		Laplacian Pyramids. As this loss function gives priority to edges over other low-frequency
# 		information, like color, it should not be used on its own. The original implementation uses this
# 		loss as a complimentary function to MSE. Ref: Optimizing the Latent Space of Generative Networks
# 		https://arxiv.org/abs/1707.05776
# 
#     - logcosh: log(cosh(x)) acts similar to MSE for small errors and to MAE for large errors. Like
# 		MSE, it is very stable and prevents overshoots when errors are near zero. Like MAE, it is robust
# 		to outliers.
# 
#     - mae: Mean absolute error will guide reconstructions of each pixel towards its median value in
# 		the training dataset. Robust to outliers but as a median, it can potentially ignore some
# 		infrequent image types in the dataset.
# 
#     - ms_ssim: Multiscale Structural Similarity Index Metric is similar to SSIM except that it
# 		performs the calculations along multiple scales of the input image.
# 
#     - mse: Mean squared error will guide reconstructions of each pixel towards its average value in
# 		the training dataset. As an avg, it will be susceptible to outliers and typically produces
# 		slightly blurrier results. Ref: Multi-Scale Structural Similarity for Image Quality Assessment
# 		https://www.cns.nyu.edu/pub/eero/wang03b.pdf
# 
#     - pixel_gradient_diff: Instead of minimizing the difference between the absolute value of each
# 		pixel in two reference images, compute the pixel to pixel spatial difference in each image and
# 		then minimize that difference between two images. Allows for large color shifts, but maintains the
# 		structure of the image.
# 
#     - smooth_loss: Smooth_L1 is a modification of the MAE loss to correct two of its disadvantages.
# 		This loss has improved stability and guidance for small errors. Ref: A General and Adaptive Robust
# 		Loss Function https://arxiv.org/pdf/1701.03077.pdf
# 
#     - ssim: Structural Similarity Index Metric is a perception-based loss that considers changes in
# 		texture, luminance, contrast, and local spatial statistics of an image. Potentially delivers more
# 		realistic looking images. Ref: Image Quality Assessment: From Error Visibility to Structural
# 		Similarity http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf
# 
# This option can be updated for existing models.
# 
# Choose from: ['ffl', 'gmsd', 'l_inf_norm', 'laploss', 'logcosh', 'mae', 'ms_ssim', 'mse',
# 'pixel_gradient_diff', 'smooth_loss', 'ssim']
# [Default: ssim]
loss_function = ssim

# The second loss function to use. If using a structural based loss (such as SSIM, MS-SSIM or GMSD) it
# is common to add an L1 regularization(MAE) or L2 regularization (MSE) function. You can adjust the
# weighting of this loss function with the loss_weight_2 option.
# 
#     - ffl: Focal Frequency Loss. Analyzes the frequency spectrum of the images rather than the
# 		images themselves. This loss function can be used on its own, but the original paper found
# 		increased benefits when using it as a complementary loss to another spacial loss function (e.g.
# 		MSE). Ref: Focal Frequency Loss for Image Reconstruction and Synthesis
# 		https://arxiv.org/pdf/2012.12821.pdf NB: This loss does not currently work on AMD cards.
# 
#     - flip: Nvidia FLIP. A perceptual loss measure that approximates the difference perceived by
# 		humans as they alternate quickly (or flip) between two images. Used on its own and this loss
# 		function creates a distinct grid on the output. However it can be helpful when used as a
# 		complimentary loss function. Ref: FLIP: A Difference Evaluator for Alternating Images:
# 		https://research.nvidia.com/sites/default/files/node/3260/FLIP_Paper.pdf
# 
#     - gmsd: Gradient Magnitude Similarity Deviation seeks to match the global standard deviation of
# 		the pixel to pixel differences between two images. Similar in approach to SSIM. Ref: Gradient
# 		Magnitude Similarity Deviation: An Highly Efficient Perceptual Image Quality Index
# 		https://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf
# 
#     - l_inf_norm: The L_inf norm will reduce the largest individual pixel error in an image. As each
# 		largest error is minimized sequentially, the overall error is improved. This loss will be
# 		extremely focused on outliers.
# 
#     - laploss: Laplacian Pyramid Loss. Attempts to improve results by focussing on edges using
# 		Laplacian Pyramids. As this loss function gives priority to edges over other low-frequency
# 		information, like color, it should not be used on its own. The original implementation uses this
# 		loss as a complimentary function to MSE. Ref: Optimizing the Latent Space of Generative Networks
# 		https://arxiv.org/abs/1707.05776
# 
#     - logcosh: log(cosh(x)) acts similar to MSE for small errors and to MAE for large errors. Like
# 		MSE, it is very stable and prevents overshoots when errors are near zero. Like MAE, it is robust
# 		to outliers.
# 
#     - lpips_alex: LPIPS is a perceptual loss that uses the feature outputs of other pretrained
# 		models as a loss metric. Be aware that this loss function will use more VRAM. Used on its own and
# 		this loss will create a distinct moire pattern on the output, however it can be helpful as a
# 		complimentary loss function. The output of this function is strong, so depending on your chosen
# 		primary loss function, you are unlikely going to want to set the weight above about 25%. Ref: The
# 		Unreasonable Effectiveness of Deep Features as a Perceptual Metric http://arxiv.org/abs/1801.03924
# This variant uses the AlexNet backbone. A fairly light and old model which performed best in the
# paper's original implementation.
# NB: For AMD Users the final linear layer is not implemented.
# 
#     - lpips_squeeze: Same as lpips_alex, but using the SqueezeNet backbone. A more lightweight
# 		version of AlexNet.
# NB: For AMD Users the final linear layer is not implemented.
# 
#     - lpips_vgg16: Same as lpips_alex, but using the VGG16 backbone. A more heavyweight model.
# NB: For AMD Users the final linear layer is not implemented.
# 
#     - mae: Mean absolute error will guide reconstructions of each pixel towards its median value in
# 		the training dataset. Robust to outliers but as a median, it can potentially ignore some
# 		infrequent image types in the dataset.
# 
#     - ms_ssim: Multiscale Structural Similarity Index Metric is similar to SSIM except that it
# 		performs the calculations along multiple scales of the input image.
# 
#     - mse: Mean squared error will guide reconstructions of each pixel towards its average value in
# 		the training dataset. As an avg, it will be susceptible to outliers and typically produces
# 		slightly blurrier results. Ref: Multi-Scale Structural Similarity for Image Quality Assessment
# 		https://www.cns.nyu.edu/pub/eero/wang03b.pdf
# 
#     - none: Do not use an additional loss function.
# 
#     - pixel_gradient_diff: Instead of minimizing the difference between the absolute value of each
# 		pixel in two reference images, compute the pixel to pixel spatial difference in each image and
# 		then minimize that difference between two images. Allows for large color shifts, but maintains the
# 		structure of the image.
# 
#     - smooth_loss: Smooth_L1 is a modification of the MAE loss to correct two of its disadvantages.
# 		This loss has improved stability and guidance for small errors. Ref: A General and Adaptive Robust
# 		Loss Function https://arxiv.org/pdf/1701.03077.pdf
# 
#     - ssim: Structural Similarity Index Metric is a perception-based loss that considers changes in
# 		texture, luminance, contrast, and local spatial statistics of an image. Potentially delivers more
# 		realistic looking images. Ref: Image Quality Assessment: From Error Visibility to Structural
# 		Similarity http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf
# 
# This option can be updated for existing models.
# 
# Choose from: ['ffl', 'flip', 'gmsd', 'l_inf_norm', 'laploss', 'logcosh', 'lpips_alex',
# 'lpips_squeeze', 'lpips_vgg16', 'mae', 'ms_ssim', 'mse', 'none', 'pixel_gradient_diff',
# 'smooth_loss', 'ssim']
# [Default: mse]
loss_function_2 = mse

# The amount of weight to apply to the second loss function.
# 
# 
# 
# The value given here is as a percentage denoting how much the selected function should contribute to
# the overall loss cost of the model. For example:
#     - 100 - The loss calculated for the second loss function will be applied at its full amount
# 		towards the overall loss score.
#     - 25 - The loss calculated for the second loss function will be reduced by a quarter prior to
# 		adding to the overall loss score.
#     - 400 - The loss calculated for the second loss function will be mulitplied 4 times prior to
# 		adding to the overall loss score.
#     - 0 - Disables the second loss function altogether.
# 
# This option can be updated for existing models.
# 
# Select an integer between 0 and 400
# [Default: 100]
loss_weight_2 = 100

# The third loss function to use. You can adjust the weighting of this loss function with the
# loss_weight_3 option.
# 
#     - ffl: Focal Frequency Loss. Analyzes the frequency spectrum of the images rather than the
# 		images themselves. This loss function can be used on its own, but the original paper found
# 		increased benefits when using it as a complementary loss to another spacial loss function (e.g.
# 		MSE). Ref: Focal Frequency Loss for Image Reconstruction and Synthesis
# 		https://arxiv.org/pdf/2012.12821.pdf NB: This loss does not currently work on AMD cards.
# 
#     - flip: Nvidia FLIP. A perceptual loss measure that approximates the difference perceived by
# 		humans as they alternate quickly (or flip) between two images. Used on its own and this loss
# 		function creates a distinct grid on the output. However it can be helpful when used as a
# 		complimentary loss function. Ref: FLIP: A Difference Evaluator for Alternating Images:
# 		https://research.nvidia.com/sites/default/files/node/3260/FLIP_Paper.pdf
# 
#     - gmsd: Gradient Magnitude Similarity Deviation seeks to match the global standard deviation of
# 		the pixel to pixel differences between two images. Similar in approach to SSIM. Ref: Gradient
# 		Magnitude Similarity Deviation: An Highly Efficient Perceptual Image Quality Index
# 		https://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf
# 
#     - l_inf_norm: The L_inf norm will reduce the largest individual pixel error in an image. As each
# 		largest error is minimized sequentially, the overall error is improved. This loss will be
# 		extremely focused on outliers.
# 
#     - laploss: Laplacian Pyramid Loss. Attempts to improve results by focussing on edges using
# 		Laplacian Pyramids. As this loss function gives priority to edges over other low-frequency
# 		information, like color, it should not be used on its own. The original implementation uses this
# 		loss as a complimentary function to MSE. Ref: Optimizing the Latent Space of Generative Networks
# 		https://arxiv.org/abs/1707.05776
# 
#     - logcosh: log(cosh(x)) acts similar to MSE for small errors and to MAE for large errors. Like
# 		MSE, it is very stable and prevents overshoots when errors are near zero. Like MAE, it is robust
# 		to outliers.
# 
#     - lpips_alex: LPIPS is a perceptual loss that uses the feature outputs of other pretrained
# 		models as a loss metric. Be aware that this loss function will use more VRAM. Used on its own and
# 		this loss will create a distinct moire pattern on the output, however it can be helpful as a
# 		complimentary loss function. The output of this function is strong, so depending on your chosen
# 		primary loss function, you are unlikely going to want to set the weight above about 25%. Ref: The
# 		Unreasonable Effectiveness of Deep Features as a Perceptual Metric http://arxiv.org/abs/1801.03924
# This variant uses the AlexNet backbone. A fairly light and old model which performed best in the
# paper's original implementation.
# NB: For AMD Users the final linear layer is not implemented.
# 
#     - lpips_squeeze: Same as lpips_alex, but using the SqueezeNet backbone. A more lightweight
# 		version of AlexNet.
# NB: For AMD Users the final linear layer is not implemented.
# 
#     - lpips_vgg16: Same as lpips_alex, but using the VGG16 backbone. A more heavyweight model.
# NB: For AMD Users the final linear layer is not implemented.
# 
#     - mae: Mean absolute error will guide reconstructions of each pixel towards its median value in
# 		the training dataset. Robust to outliers but as a median, it can potentially ignore some
# 		infrequent image types in the dataset.
# 
#     - ms_ssim: Multiscale Structural Similarity Index Metric is similar to SSIM except that it
# 		performs the calculations along multiple scales of the input image.
# 
#     - mse: Mean squared error will guide reconstructions of each pixel towards its average value in
# 		the training dataset. As an avg, it will be susceptible to outliers and typically produces
# 		slightly blurrier results. Ref: Multi-Scale Structural Similarity for Image Quality Assessment
# 		https://www.cns.nyu.edu/pub/eero/wang03b.pdf
# 
#     - none: Do not use an additional loss function.
# 
#     - pixel_gradient_diff: Instead of minimizing the difference between the absolute value of each
# 		pixel in two reference images, compute the pixel to pixel spatial difference in each image and
# 		then minimize that difference between two images. Allows for large color shifts, but maintains the
# 		structure of the image.
# 
#     - smooth_loss: Smooth_L1 is a modification of the MAE loss to correct two of its disadvantages.
# 		This loss has improved stability and guidance for small errors. Ref: A General and Adaptive Robust
# 		Loss Function https://arxiv.org/pdf/1701.03077.pdf
# 
#     - ssim: Structural Similarity Index Metric is a perception-based loss that considers changes in
# 		texture, luminance, contrast, and local spatial statistics of an image. Potentially delivers more
# 		realistic looking images. Ref: Image Quality Assessment: From Error Visibility to Structural
# 		Similarity http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf
# 
# This option can be updated for existing models.
# 
# Choose from: ['ffl', 'flip', 'gmsd', 'l_inf_norm', 'laploss', 'logcosh', 'lpips_alex',
# 'lpips_squeeze', 'lpips_vgg16', 'mae', 'ms_ssim', 'mse', 'none', 'pixel_gradient_diff',
# 'smooth_loss', 'ssim']
# [Default: none]
loss_function_3 = none

# The amount of weight to apply to the third loss function.
# 
# 
# 
# The value given here is as a percentage denoting how much the selected function should contribute to
# the overall loss cost of the model. For example:
#     - 100 - The loss calculated for the third loss function will be applied at its full amount
# 		towards the overall loss score.
#     - 25 - The loss calculated for the third loss function will be reduced by a quarter prior to
# 		adding to the overall loss score.
#     - 400 - The loss calculated for the third loss function will be mulitplied 4 times prior to
# 		adding to the overall loss score.
#     - 0 - Disables the third loss function altogether.
# 
# This option can be updated for existing models.
# 
# Select an integer between 0 and 400
# [Default: 0]
loss_weight_3 = 0

# The fourth loss function to use. You can adjust the weighting of this loss function with the
# loss_weight_3 option.
# 
#     - ffl: Focal Frequency Loss. Analyzes the frequency spectrum of the images rather than the
# 		images themselves. This loss function can be used on its own, but the original paper found
# 		increased benefits when using it as a complementary loss to another spacial loss function (e.g.
# 		MSE). Ref: Focal Frequency Loss for Image Reconstruction and Synthesis
# 		https://arxiv.org/pdf/2012.12821.pdf NB: This loss does not currently work on AMD cards.
# 
#     - flip: Nvidia FLIP. A perceptual loss measure that approximates the difference perceived by
# 		humans as they alternate quickly (or flip) between two images. Used on its own and this loss
# 		function creates a distinct grid on the output. However it can be helpful when used as a
# 		complimentary loss function. Ref: FLIP: A Difference Evaluator for Alternating Images:
# 		https://research.nvidia.com/sites/default/files/node/3260/FLIP_Paper.pdf
# 
#     - gmsd: Gradient Magnitude Similarity Deviation seeks to match the global standard deviation of
# 		the pixel to pixel differences between two images. Similar in approach to SSIM. Ref: Gradient
# 		Magnitude Similarity Deviation: An Highly Efficient Perceptual Image Quality Index
# 		https://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf
# 
#     - l_inf_norm: The L_inf norm will reduce the largest individual pixel error in an image. As each
# 		largest error is minimized sequentially, the overall error is improved. This loss will be
# 		extremely focused on outliers.
# 
#     - laploss: Laplacian Pyramid Loss. Attempts to improve results by focussing on edges using
# 		Laplacian Pyramids. As this loss function gives priority to edges over other low-frequency
# 		information, like color, it should not be used on its own. The original implementation uses this
# 		loss as a complimentary function to MSE. Ref: Optimizing the Latent Space of Generative Networks
# 		https://arxiv.org/abs/1707.05776
# 
#     - logcosh: log(cosh(x)) acts similar to MSE for small errors and to MAE for large errors. Like
# 		MSE, it is very stable and prevents overshoots when errors are near zero. Like MAE, it is robust
# 		to outliers.
# 
#     - lpips_alex: LPIPS is a perceptual loss that uses the feature outputs of other pretrained
# 		models as a loss metric. Be aware that this loss function will use more VRAM. Used on its own and
# 		this loss will create a distinct moire pattern on the output, however it can be helpful as a
# 		complimentary loss function. The output of this function is strong, so depending on your chosen
# 		primary loss function, you are unlikely going to want to set the weight above about 25%. Ref: The
# 		Unreasonable Effectiveness of Deep Features as a Perceptual Metric http://arxiv.org/abs/1801.03924
# This variant uses the AlexNet backbone. A fairly light and old model which performed best in the
# paper's original implementation.
# NB: For AMD Users the final linear layer is not implemented.
# 
#     - lpips_squeeze: Same as lpips_alex, but using the SqueezeNet backbone. A more lightweight
# 		version of AlexNet.
# NB: For AMD Users the final linear layer is not implemented.
# 
#     - lpips_vgg16: Same as lpips_alex, but using the VGG16 backbone. A more heavyweight model.
# NB: For AMD Users the final linear layer is not implemented.
# 
#     - mae: Mean absolute error will guide reconstructions of each pixel towards its median value in
# 		the training dataset. Robust to outliers but as a median, it can potentially ignore some
# 		infrequent image types in the dataset.
# 
#     - ms_ssim: Multiscale Structural Similarity Index Metric is similar to SSIM except that it
# 		performs the calculations along multiple scales of the input image.
# 
#     - mse: Mean squared error will guide reconstructions of each pixel towards its average value in
# 		the training dataset. As an avg, it will be susceptible to outliers and typically produces
# 		slightly blurrier results. Ref: Multi-Scale Structural Similarity for Image Quality Assessment
# 		https://www.cns.nyu.edu/pub/eero/wang03b.pdf
# 
#     - none: Do not use an additional loss function.
# 
#     - pixel_gradient_diff: Instead of minimizing the difference between the absolute value of each
# 		pixel in two reference images, compute the pixel to pixel spatial difference in each image and
# 		then minimize that difference between two images. Allows for large color shifts, but maintains the
# 		structure of the image.
# 
#     - smooth_loss: Smooth_L1 is a modification of the MAE loss to correct two of its disadvantages.
# 		This loss has improved stability and guidance for small errors. Ref: A General and Adaptive Robust
# 		Loss Function https://arxiv.org/pdf/1701.03077.pdf
# 
#     - ssim: Structural Similarity Index Metric is a perception-based loss that considers changes in
# 		texture, luminance, contrast, and local spatial statistics of an image. Potentially delivers more
# 		realistic looking images. Ref: Image Quality Assessment: From Error Visibility to Structural
# 		Similarity http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf
# 
# This option can be updated for existing models.
# 
# Choose from: ['ffl', 'flip', 'gmsd', 'l_inf_norm', 'laploss', 'logcosh', 'lpips_alex',
# 'lpips_squeeze', 'lpips_vgg16', 'mae', 'ms_ssim', 'mse', 'none', 'pixel_gradient_diff',
# 'smooth_loss', 'ssim']
# [Default: none]
loss_function_4 = none

# The amount of weight to apply to the fourth loss function.
# 
# 
# 
# The value given here is as a percentage denoting how much the selected function should contribute to
# the overall loss cost of the model. For example:
#     - 100 - The loss calculated for the fourth loss function will be applied at its full amount
# 		towards the overall loss score.
#     - 25 - The loss calculated for the fourth loss function will be reduced by a quarter prior to
# 		adding to the overall loss score.
#     - 400 - The loss calculated for the fourth loss function will be mulitplied 4 times prior to
# 		adding to the overall loss score.
#     - 0 - Disables the fourth loss function altogether.
# 
# This option can be updated for existing models.
# 
# Select an integer between 0 and 400
# [Default: 0]
loss_weight_4 = 0

# The loss function to use when learning a mask.
#     - MAE - Mean absolute error will guide reconstructions of each pixel towards its median value in
# 		the training dataset. Robust to outliers but as a median, it can potentially ignore some
# 		infrequent image types in the dataset.
#     - MSE - Mean squared error will guide reconstructions of each pixel towards its average value in
# 		the training dataset. As an average, it will be susceptible to outliers and typically produces
# 		slightly blurrier results.
# 
# This option can be updated for existing models.
# 
# Choose from: ['mae', 'mse']
# [Default: mse]
mask_loss_function = mse

# The amount of priority to give to the eyes.
# 
# The value given here is as a multiplier of the main loss score. For example:
#     - 1 - The eyes will receive the same priority as the rest of the face.
#     - 10 - The eyes will be given a score 10 times higher than the rest of the face.
# 
# NB: Penalized Mask Loss must be enable to use this option.
# 
# This option can be updated for existing models.
# 
# Select an integer between 1 and 40
# [Default: 3]
eye_multiplier = 3

# The amount of priority to give to the mouth.
# 
# The value given here is as a multiplier of the main loss score. For Example:
#     - 1 - The mouth will receive the same priority as the rest of the face.
#     - 10 - The mouth will be given a score 10 times higher than the rest of the face.
# 
# NB: Penalized Mask Loss must be enable to use this option.
# 
# This option can be updated for existing models.
# 
# Select an integer between 1 and 40
# [Default: 2]
mouth_multiplier = 2

# Image loss function is weighted by mask presence. For areas of the image without the facial mask,
# reconstruction errors will be ignored while the masked face area is prioritized. May increase
# overall quality by focusing attention on the core face area.
# 
# Choose from: True, False
# [Default: True]
penalized_mask_loss = True

# The mask to be used for training. If you have selected 'Learn Mask' or 'Penalized Mask Loss' you
# must select a value other than 'none'. The required mask should have been selected as part of the
# Extract process. If it does not exist in the alignments file then it will be generated prior to
# training commencing.
#     - none: Don't use a mask.
#     - bisenet-fp_face: Relatively lightweight NN based mask that provides more refined control over
# 		the area to be masked (configurable in mask settings). Use this version of bisenet-fp if your
# 		model is trained with 'face' or 'legacy' centering.
#     - bisenet-fp_head: Relatively lightweight NN based mask that provides more refined control over
# 		the area to be masked (configurable in mask settings). Use this version of bisenet-fp if your
# 		model is trained with 'head' centering.
#     - components: Mask designed to provide facial segmentation based on the positioning of landmark
# 		locations. A convex hull is constructed around the exterior of the landmarks to create a mask.
#     - custom_face: Custom user created, face centered mask.
#     - custom_head: Custom user created, head centered mask.
#     - extended: Mask designed to provide facial segmentation based on the positioning of landmark
# 		locations. A convex hull is constructed around the exterior of the landmarks and the mask is
# 		extended upwards onto the forehead.
#     - vgg-clear: Mask designed to provide smart segmentation of mostly frontal faces clear of
# 		obstructions. Profile faces and obstructions may result in sub-par performance.
#     - vgg-obstructed: Mask designed to provide smart segmentation of mostly frontal faces. The mask
# 		model has been specifically trained to recognize some facial obstructions (hands and eyeglasses).
# 		Profile faces may result in sub-par performance.
#     - unet-dfl: Mask designed to provide smart segmentation of mostly frontal faces. The mask model
# 		has been trained by community members and will need testing for further description. Profile faces
# 		may result in sub-par performance.
# 
# Choose from: ['none', 'bisenet-fp_face', 'bisenet-fp_head', 'components', 'custom_face',
# 'custom_head', 'extended', 'unet-dfl', 'vgg-clear', 'vgg-obstructed']
# [Default: extended]
mask_type = extended

# Dilate or erode the mask. Negative values erode the mask (make it smaller). Positive values dilate
# the mask (make it larger). The value given is a percentage of the total mask size.
# 
# This option can be updated for existing models.
# 
# Select a decimal number between -5.0 and 5.0
# [Default: 0]
mask_dilation = 0

# Apply gaussian blur to the mask input. This has the effect of smoothing the edges of the mask, which
# can help with poorly calculated masks and give less of a hard edge to the predicted mask. The size
# is in pixels (calculated from a 128px mask). Set to 0 to not apply gaussian blur. This value should
# be odd, if an even number is passed in then it will be rounded to the next odd number.
# 
# This option can be updated for existing models.
# 
# Select an integer between 0 and 9
# [Default: 3]
mask_blur_kernel = 3

# Sets pixels that are near white to white and near black to black. Set to 0 for off.
# 
# This option can be updated for existing models.
# 
# Select an integer between 0 and 50
# [Default: 4]
mask_threshold = 4

# Dedicate a portion of the model to learning how to duplicate the input mask. Increases VRAM usage in
# exchange for learning a quick ability to try to replicate more complex mask models.
# 
# Choose from: True, False
# [Default: False]
learn_mask = False

[model.dfl_sae]
# DFL SAE MODEL (ADAPTED FROM HTTPS://GITHUB.COM/IPEROV/DEEPFACELAB)

# Resolution (in pixels) of the input image to train on.
# BE AWARE Larger resolution will dramatically increase VRAM requirements.
# 
# Must be divisible by 16.
# 
# Select an integer between 64 and 256
# [Default: 128]
input_size = 128

# Model architecture:
#     - 'df': Keeps the faces more natural.
#     - 'liae': Can help fix overly different face shapes.
# 
# Choose from: ['df', 'liae']
# [Default: df]
architecture = df

# Face information is stored in AutoEncoder dimensions. If there are not enough dimensions then
# certain facial features may not be recognized.
# Higher number of dimensions are better, but require more VRAM.
# Set to 0 to use the architecture defaults (256 for liae, 512 for df).
# 
# Select an integer between 0 and 1024
# [Default: 0]
autoencoder_dims = 0

# Encoder dimensions per channel. Higher number of encoder dimensions will help the model to recognize
# more facial features, but will require more VRAM.
# 
# Select an integer between 21 and 85
# [Default: 42]
encoder_dims = 42

# Decoder dimensions per channel. Higher number of decoder dimensions will help the model to improve
# details, but will require more VRAM.
# 
# Select an integer between 10 and 85
# [Default: 21]
decoder_dims = 21

# Multiscale decoder can help to obtain better details.
# 
# Choose from: True, False
# [Default: False]
multiscale_decoder = False

[model.dlight]
# A LIGHTWEIGHT, HIGH RESOLUTION DFAKER VARIANT (ADAPTED FROM HTTPS://GITHUB.COM/DFAKER/DF)

# Higher settings will allow learning more features such as tatoos, piercing and wrinkles.
# Strongly affects VRAM usage.
# 
# Choose from: ['lowmem', 'fair', 'best']
# [Default: best]
features = best

# Defines detail fidelity. Lower setting can appear 'rugged' while 'good' might take a longer time to
# train.
# Affects VRAM usage.
# 
# Choose from: ['fast', 'good']
# [Default: good]
details = good

# Output image resolution (in pixels).
# Be aware that larger resolution will increase VRAM requirements.
# NB: Must be either 128, 256, or 384.
# 
# Select an integer between 128 and 384
# [Default: 256]
output_size = 256

[model.phaze_a]
# PHAZE-A MODEL BY TORZDF, WITH THANKS TO BIRBFAKES.
# ALLOWS FOR THE EXPERIMENTATION OF VARIOUS STANDARD NETWORKS AS THE ENCODER AND TAKES INSPIRATION
# FROM NVIDIA'S STYLEGAN FOR THE DECODER. IT IS HIGHLY RECOMMENDED TO RESEARCH TO UNDERSTAND THE
# PARAMETERS BETTER.

# Resolution (in pixels) of the output image to generate.
# BE AWARE Larger resolution will dramatically increase VRAM requirements.
# 
# Select an integer between 64 and 2048
# [Default: 128]
output_size = 128

# Whether to create a shared fully connected layer. This layer will have the same structure as the
# fully connected layers used for each side of the model. A shared fully connected layer looks for
# patterns that are common to both sides. NB: Enabling this option only makes sense if 'split fc' is
# selected.
#     - none - Do not create a Fully Connected layer for shared data. (Original method)
#     - full - Create an exclusive Fully Connected layer for shared data. (IAE method)
#     - half - Use the 'fc_a' layer for shared data. This saves VRAM by re-using the 'A' side's fully
# 		connected model for the shared data. However, this will lead to an 'unbalanced' model and can lead
# 		to more identity bleed (DFL method)
# 
# Choose from: ['none', 'full', 'half']
# [Default: none]
shared_fc = none

# Whether to enable the G-Block. If enabled, this will create a shared fully connected layer
# (configurable in the 'G-Block hidden layers' section) to look for patterns in the combined data,
# before feeding a block prior to the decoder for merging this shared and combined data.
#     - True - Use the G-Block in the Decoder. A combined fully connected layer will be created to
# 		feed this block which can be configured below.
#     - False - Don't use the G-Block in the decoder. No combined fully connected layer will be
# 		created.
# 
# Choose from: True, False
# [Default: True]
enable_gblock = True

# Whether to use a single shared Fully Connected layer or separate Fully Connected layers for each
# side.
#     - True - Use separate Fully Connected layers for Face A and Face B. This is more similar to the
# 		'IAE' style of model.
#     - False - Use combined Fully Connected layers for both sides. This is more similar to the
# 		original Faceswap architecture.
# 
# Choose from: True, False
# [Default: True]
split_fc = True

# If the G-Block is enabled, Whether to use a single G-Block shared between both sides, or whether to
# have a separate G-Block (one for each side). NB: The Fully Connected layer that feeds the G-Block
# will always be shared.
#     - True - Use separate G-Blocks for Face A and Face B.
#     - False - Use a combined G-Block layers for both sides.
# 
# Choose from: True, False
# [Default: False]
split_gblock = False

# Whether to use a single decoder or split decoders.
#     - True - Use a separate decoder for Face A and Face B. This is more similar to the original
# 		Faceswap architecture.
#     - False - Use a combined Decoder. This is more similar to 'IAE' style architecture.
# 
# Choose from: True, False
# [Default: False]
split_decoders = False

# The encoder architecture to use. See the relevant config sections for specific architecture
# tweaking.
# NB: For keras based pre-built models, the global initializers and padding options will be ignored
# for the selected encoder.
# 
#     - CLIPv: This is an implementation of the Visual encoder from the CLIP transformer. The ViT
# 		weights are trained on imagenet whilst the FaRL weights are trained on face related tasks. All
# 		have a default input size of 224px except for ViT-L-14-336px that has an input size of 336px. Ref:
# 		Learning Transferable Visual Models From Natural Language Supervision (2021):
# 		https://arxiv.org/abs/2103.00020
# 
#     - densenet: (32px -224px). Ref: Densely Connected Convolutional Networks (2016):
# 		https://arxiv.org/abs/1608.06993?source=post_page
# 
#     - efficientnet: [Tensorflow 2.3+ only] EfficientNet has numerous variants (B0 - B8) that
# 		increases the model width, depth and dimensional space at each step. The minimum input resolution
# 		is 32px for all variants. The maximum input resolution for each variant is: b0: 224px, b1: 240px,
# 		b2: 260px, b3: 300px, b4: 380px, b5: 456px, b6: 528px, b7 600px. Ref: Rethinking Model Scaling for
# 		Convolutional Neural Networks (2020): https://arxiv.org/abs/1905.11946
# 
#     - efficientnet_v2: [Tensorflow 2.8+ only] EfficientNetV2 is the follow up to efficientnet. It
# 		has numerous variants (B0 - B3 and Small, Medium and Large) that increases the model width, depth
# 		and dimensional space at each step. The minimum input resolution is 32px for all variants. The
# 		maximum input resolution for each variant is: b0: 224px, b1: 240px, b2: 260px, b3: 300px, s:
# 		384px, m: 480px, l: 480px. Ref: EfficientNetV2: Smaller Models and Faster Training (2021):
# 		https://arxiv.org/abs/2104.00298
# 
#     - fs_original: (32px - 1024px). A configurable variant of the original facewap encoder. ImageNet
# 		weights cannot be loaded for this model. Additional parameters can be configured with the 'fs_enc'
# 		options. A version of this encoder is used in the following models: Original, Original (lowmem),
# 		Dfaker, DFL-H128, DFL-SAE, IAE, Lightweight.
# 
#     - inception_resnet_v2: (75px - 299px). Ref: Inception-ResNet and the Impact of Residual
# 		Connections on Learning (2016): https://arxiv.org/abs/1602.07261
# 
#     - inceptionV3: (75px - 299px). Ref: Rethinking the Inception Architecture for Computer Vision
# 		(2015): https://arxiv.org/abs/1512.00567
# 
#     - mobilenet: (32px - 224px). Additional MobileNet parameters can be set with the 'mobilenet'
# 		options. Ref: MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
# 		(2017): https://arxiv.org/abs/1704.04861
# 
#     - mobilenet_v2: (32px - 224px). Additional MobileNet parameters can be set with the 'mobilenet'
# 		options. Ref: MobileNetV2: Inverted Residuals and Linear Bottlenecks (2018):
# 		https://arxiv.org/abs/1801.04381
# 
#     - mobilenet_v3: (32px - 224px). Additional MobileNet parameters can be set with the 'mobilenet'
# 		options. Ref: Searching for MobileNetV3 (2019): https://arxiv.org/pdf/1905.02244.pdf
# 
#     - nasnet: (32px - 331px (large) or 224px (mobile)). Ref: Learning Transferable Architectures for
# 		Scalable Image Recognition (2017): https://arxiv.org/abs/1707.07012
# 
#     - resnet: (32px - 224px). Deep Residual Learning for Image Recognition (2015):
# 		https://arxiv.org/abs/1512.03385
# 
#     - vgg: (32px - 224px). Very Deep Convolutional Networks for Large-Scale Image Recognition
# 		(2014): https://arxiv.org/abs/1409.1556
# 
#     - xception: (71px - 229px). Ref: Deep Learning with Depthwise Separable Convolutions (2017):
# 		https://arxiv.org/abs/1409.1556.
# 
# 
# Choose from: ['clipv_farl-b-16-16', 'clipv_farl-b-16-64', 'clipv_vit-b-16', 'clipv_vit-b-32',
# 'clipv_vit-l-14', 'clipv_vit-l-14-336px', 'densenet121', 'densenet169', 'densenet201',
# 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4',
# 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_v2_b0', 'efficientnet_v2_b1',
# 'efficientnet_v2_b2', 'efficientnet_v2_b3', 'efficientnet_v2_l', 'efficientnet_v2_m',
# 'efficientnet_v2_s', 'fs_original', 'inception_resnet_v2', 'inception_v3', 'mobilenet',
# 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'nasnet_large', 'nasnet_mobile',
# 'resnet101', 'resnet101_v2', 'resnet152', 'resnet152_v2', 'resnet50', 'resnet50_v2', 'vgg16',
# 'vgg19', 'xception']
# [Default: fs_original]
enc_architecture = fs_original

# Input scaling for the encoder. Some of the encoders have large input sizes, which often are not
# helpful for Faceswap. This setting scales the dimensional space that the encoder works in. For
# example an encoder with a maximum input size of 224px will be input an image of 112px at 50%%
# scaling. See the Architecture tooltip for the minimum and maximum sizes for each encoder. NB: The
# input size will be rounded down to the nearest 16 pixels.
# 
# Select an integer between 0 and 200
# [Default: 7]
enc_scaling = 7

# Load pre-trained weights trained on ImageNet data. Only available for non-Faceswap encoders (i.e.
# those not beginning with 'fs'). NB: If you use the global 'load weights' option and have selected to
# load weights from a previous model's 'encoder' or 'keras_encoder' then the weights loaded here will
# be replaced by the weights loaded from your saved model.
# 
# Choose from: True, False
# [Default: True]
enc_load_weights = True

# The type of layer to use for the bottleneck.
#     - average_pooling: Use a Global Average Pooling 2D layer for the bottleneck.
#     - dense: Use a Dense layer for the bottleneck (the traditional Faceswap method). You can set the
# 		size of the Dense layer with the 'bottleneck_size' parameter.
#     - max_pooling: Use a Global Max Pooling 2D layer for the bottleneck.
#  latten: Don't use a bottleneck at all. Some encoders output in a size that make a bottleneck
# unnecessary. This option flattens the output from the encoder, with no further operations
# 
# Choose from: ['average_pooling', 'dense', 'max_pooling', 'flatten']
# [Default: dense]
bottleneck_type = dense

# Apply a normalization layer after encoder output and prior to the bottleneck.
#     - none - Do not apply a normalization layer
#     - instance - Apply Instance Normalization
#     - layer - Apply Layer Normalization (Ba et al., 2016)
#     - rms - Apply Root Mean Squared Layer Normalization (Zhang et al., 2019). A simplified version
# 		of Layer Normalization with reduced overhead.
# 
# Choose from: ['none', 'instance', 'layer', 'rms']
# [Default: none]
bottleneck_norm = none

# If using a Dense layer for the bottleneck, then this is the number of nodes to use.
# 
# Select an integer between 128 and 4096
# [Default: 1024]
bottleneck_size = 1024

# Whether to place the bottleneck in the Encoder or to place it with the other hidden layers. Placing
# the bottleneck in the encoder means that both sides will share the same bottleneck. Placing it with
# the other fully connected layers means that each fully connected layer will each get their own
# bottleneck. This may be combined or split depending on your overall architecture configuration
# settings.
# 
# Choose from: True, False
# [Default: True]
bottleneck_in_encoder = True

# The number of consecutive Dense (fully connected) layers to include in each side's intermediate
# layer.
# 
# Select an integer between 0 and 16
# [Default: 1]
fc_depth = 1

# The number of filters to use for the initial fully connected layer. The number of nodes actually
# used is: fc_min_filters x fc_dimensions x fc_dimensions.
# NB: This value may be scaled down, depending on output resolution.
# 
# Select an integer between 16 and 5120
# [Default: 1024]
fc_min_filters = 1024

# This is the number of filters to be used in the final reshape layer at the end of the fully
# connected layers. The actual number of nodes used for the final fully connected layer is:
# fc_min_filters x fc_dimensions x fc_dimensions.
# NB: This value may be scaled down, depending on output resolution.
# 
# Select an integer between 128 and 5120
# [Default: 1024]
fc_max_filters = 1024

# The height and width dimension for the final reshape layer at the end of the fully connected layers.
# NB: The total number of nodes within the final fully connected layer will be: fc_dimensions x
# fc_dimensions x fc_max_filters.
# 
# Select an integer between 1 and 16
# [Default: 4]
fc_dimensions = 4

# The rate that the filters move from the minimum number of filters to the maximum number of filters.
# EG:
# Negative numbers will change the number of filters quicker at first and slow down each layer.
# Positive numbers will change the number of filters slower at first but then speed up each layer.
# 0.0 - This will change at a linear rate (i.e. the same number of filters will be changed at each
# layer).
# 
# Select a decimal number between -0.99 and 0.99
# [Default: -0.5]
fc_filter_slope = -0.5

# Dropout is a form of regularization that can prevent a model from over-fitting and help to keep
# neurons 'alive'. 0.5 will dropout half the connections between each fully connected layer, 0.25 will
# dropout a quarter of the connections etc. Set to 0.0 to disable.
# 
# This option can be updated for existing models.
# 
# Select a decimal number between 0.0 and 0.99
# [Default: 0.0]
fc_dropout = 0.0

# The type of dimensional upsampling to perform at the end of the fully connected layers, if upsamples
# > 0. The number of filters used for the upscale layers will be the value given in
# 'fc_upsample_filters'.
#     - upsample2d - A lightweight and VRAM friendly method. 'quick and dirty' but does not learn any
# 		parameters
#     - subpixel - Sub-pixel upscaler using depth-to-space which may require more VRAM.
#     - resize_images - Uses the Keras resize_image function to save about half as much vram as the
# 		heaviest methods.
#     - upscale_fast - Developed by Andenixa. Focusses on speed to upscale, but requires more VRAM.
#     - upscale_hybrid - Developed by Andenixa. Uses a combination of PixelShuffler and Upsampling2D
# 		to upscale, saving about 1/3rd of VRAM of the heaviest methods.
# 
# Choose from: ['resize_images', 'subpixel', 'upscale_fast', 'upscale_hybrid', 'upsample2d']
# [Default: upsample2d]
fc_upsampler = upsample2d

# Some upsampling can occur within the Fully Connected layers rather than in the Decoder to increase
# the dimensional space. Set how many upscale layers should occur within the Fully Connected layers.
# 
# Select an integer between 0 and 4
# [Default: 1]
fc_upsamples = 1

# If you have selected an upsampler which requires filters (i.e. any upsampler with the exception of
# Upsampling2D), then this is the number of filters to be used for the upsamplers within the fully
# connected layers,  NB: This value may be scaled down, depending on output resolution. Also note,
# that this figure will dictate the number of filters used for the G-Block, if selected.
# 
# Select an integer between 128 and 5120
# [Default: 512]
fc_upsample_filters = 512

# The number of consecutive Dense (fully connected) layers to include in the G-Block shared layer.
# 
# Select an integer between 1 and 16
# [Default: 3]
fc_gblock_depth = 3

# The number of nodes to use for the initial G-Block shared fully connected layer.
# 
# Select an integer between 128 and 5120
# [Default: 512]
fc_gblock_min_nodes = 512

# The number of nodes to use for the final G-Block shared fully connected layer.
# 
# Select an integer between 128 and 5120
# [Default: 512]
fc_gblock_max_nodes = 512

# The rate that the filters move from the minimum number of filters to the maximum number of filters
# for the G-Block shared layers. EG:
# Negative numbers will change the number of filters quicker at first and slow down each layer.
# Positive numbers will change the number of filters slower at first but then speed up each layer.
# 0.0 - This will change at a linear rate (i.e. the same number of filters will be changed at each
# layer).
# 
# Select a decimal number between -0.99 and 0.99
# [Default: -0.5]
fc_gblock_filter_slope = -0.5

# Dropout is a regularization technique that can prevent a model from over-fitting and help to keep
# neurons 'alive'. 0.5 will dropout half the connections between each fully connected layer, 0.25 will
# dropout a quarter of the connections etc. Set to 0.0 to disable.
# 
# This option can be updated for existing models.
# 
# Select a decimal number between 0.0 and 0.99
# [Default: 0.0]
fc_gblock_dropout = 0.0

# The method to use for the upscales within the decoder. Images are upscaled multiple times within the
# decoder as the network learns to reconstruct the face.
#     - subpixel - Sub-pixel upscaler using depth-to-space which requires more VRAM.
#     - resize_images - Uses the Keras resize_image function to save about half as much vram as the
# 		heaviest methods.
#     - upscale_fast - Developed by Andenixa. Focusses on speed to upscale, but requires more VRAM.
#     - upscale_hybrid - Developed by Andenixa. Uses a combination of PixelShuffler and Upsampling2D
# 		to upscale, saving about 1/3rd of VRAM of the heaviest methods.
#     - upscale_dny - An alternative upscale implementation using Upsampling2D to upsale.
# 
# Choose from: ['subpixel', 'resize_images', 'upscale_fast', 'upscale_hybrid', 'upscale_dny']
# [Default: subpixel]
dec_upscale_method = subpixel

# It is possible to place some of the upscales at the end of the fully connected model. For models
# with split decoders, but a shared fully connected layer, this would have the effect of saving some
# VRAM but possibly at the cost of introducing artefacts. For models with a shared decoder but split
# fully connected layers, this would have the effect of increasing VRAM usage by processing some of
# the upscales for each side rather than together.
# 
# Select an integer between 0 and 6
# [Default: 0]
dec_upscales_in_fc = 0

# Normalization to apply to apply after each upscale.
#     - none - Do not apply a normalization layer
#     - batch - Apply Batch Normalization
#     - group - Apply Group Normalization
#     - instance - Apply Instance Normalization
#     - layer - Apply Layer Normalization (Ba et al., 2016)
#     - rms - Apply Root Mean Squared Layer Normalization (Zhang et al., 2019). A simplified version
# 		of Layer Normalization with reduced overhead.
# 
# Choose from: ['none', 'batch', 'group', 'instance', 'layer', 'rms']
# [Default: none]
dec_norm = none

# The minimum number of filters to use in decoder upscalers (i.e. the number of filters to use for the
# final upscale layer).
# 
# Select an integer between 16 and 512
# [Default: 64]
dec_min_filters = 64

# The maximum number of filters to use in decoder upscalers (i.e. the number of filters to use for the
# first upscale layer).
# 
# Select an integer between 256 and 5120
# [Default: 512]
dec_max_filters = 512

# Alters the action of the filter slope.
# 
#     - full: The number of filters at each upscale layer will reduce from the chosen max_filters at
# 		the first layer to the chosen min_filters at the last layer as dictated by the dec_filter_slope.
#     - cap_max: The filters will decline at a fixed rate from each upscale to the next based on the
# 		filter_slope setting. If there are more upscales than filters, then the earliest upscales will be
# 		capped at the max_filter value until the filters can reduce to the min_filters value at the final
# 		upscale. (EG: 512 -> 512 -> 512 -> 256 -> 128 -> 64).
#     - cap_min: The filters will decline at a fixed rate from each upscale to the next based on the
# 		filter_slope setting. If there are more upscales than filters, then the earliest upscales will
# 		drop their filters until the min_filter value is met and repeat the min_filter value for the
# 		remaining upscales. (EG: 512 -> 256 -> 128 -> 64 -> 64 -> 64).
# 
# Choose from: ['full', 'cap_max', 'cap_min']
# [Default: full]
dec_slope_mode = full

# The rate that the filters reduce at each upscale layer.
# 
#     - Full Slope Mode: Negative numbers will drop the number of filters quicker at first and slow
# 		down each upscale. Positive numbers will drop the number of filters slower at first but then speed
# 		up each upscale. A value of 0.0 will reduce at a linear rate (i.e. the same number of filters will
# 		be reduced at each upscale).
# 
#     - Cap Min/Max Slope Mode: Only positive values will work here. Negative values will
# 		automatically be converted to their positive counterpart. A value of 0.5 will halve the number of
# 		filters at each upscale until the minimum value is reached. A value of 0.33 will be reduce the
# 		number of filters by a third until the minimum value is reached etc.
# 
# Select a decimal number between -0.99 and 0.99
# [Default: -0.45]
dec_filter_slope = -0.45

# The number of Residual Blocks to apply to each upscale layer. Set to 0 to disable residual blocks
# entirely.
# 
# Select an integer between 0 and 8
# [Default: 1]
dec_res_blocks = 1

# The kernel size to apply to the final Convolution layer.
# 
# Select an integer between 1 and 9
# [Default: 5]
dec_output_kernel = 5

# Gaussian Noise acts as a regularization technique for preventing overfitting of data.
#     - True - Apply a Gaussian Noise layer to each upscale.
#     - False - Don't apply a Gaussian Noise layer to each upscale.
# 
# Choose from: True, False
# [Default: True]
dec_gaussian = True

# If Residual blocks have been enabled, enabling this option will not apply a Residual block to the
# final upscaler.
#     - True - Don't apply a Residual block to the final upscale.
#     - False - Apply a Residual block to all upscale layers.
# 
# Choose from: True, False
# [Default: True]
dec_skip_last_residual = True

# If the command line option 'freeze-weights' is enabled, then the layers indicated here will be
# frozen the next time the model starts up. NB: Not all architectures contain all of the layers listed
# here, so any layers marked for freezing that are not within your chosen architecture will be
# ignored. EG:
#  If 'split fc' has been selected, then 'fc_a' and 'fc_b' are available for freezing. If it has not
# been selected then 'fc_both' is available for freezing.
# 
# This option can be updated for existing models.
# 
# If selecting multiple options then each option should be separated by a space or a comma (e.g.
# item1, item2, item3)
# 
# Choose from: ['encoder', 'keras_encoder', 'fc_a', 'fc_b', 'fc_both', 'fc_shared', 'fc_gblock',
# 'g_block_a', 'g_block_b', 'g_block_both', 'decoder_a', 'decoder_b', 'decoder_both']
# [Default: keras_encoder]
freeze_layers = keras_encoder

# If the command line option 'load-weights' is populated, then the layers indicated here will be
# loaded from the given weights file if starting a new model. NB Not all architectures contain all of
# the layers listed here, so any layers marked for loading that are not within your chosen
# architecture will be ignored. EG:
#  If 'split fc' has been selected, then 'fc_a' and 'fc_b' are available for loading. If it has not
# been selected then 'fc_both' is available for loading.
# 
# If selecting multiple options then each option should be separated by a space or a comma (e.g.
# item1, item2, item3)
# 
# Choose from: ['encoder', 'fc_a', 'fc_b', 'fc_both', 'fc_shared', 'fc_gblock', 'g_block_a',
# 'g_block_b', 'g_block_both', 'decoder_a', 'decoder_b', 'decoder_both']
# [Default: encoder]
load_layers = encoder

# Faceswap Encoder only: The number of convolutions to perform within the encoder.
# 
# Select an integer between 2 and 10
# [Default: 4]
fs_original_depth = 4

# Faceswap Encoder only: The minumum number of filters to use for encoder convolutions. (i.e. the
# number of filters to use for the first encoder layer).
# 
# Select an integer between 16 and 2048
# [Default: 128]
fs_original_min_filters = 128

# Faceswap Encoder only: The maximum number of filters to use for encoder convolutions. (i.e. the
# number of filters to use for the final encoder layer).
# 
# Select an integer between 256 and 8192
# [Default: 1024]
fs_original_max_filters = 1024

# Use a slightly alternate version of the Faceswap Encoder.
#     - True - Use the alternate variation of the Faceswap Encoder.
#     - False - Use the original Faceswap Encoder.
# 
# Choose from: True, False
# [Default: False]
fs_original_use_alt = False

# The width multiplier for mobilenet encoders. Controls the width of the network. Values less than 1.0
# proportionally decrease the number of filters within each layer. Values greater than 1.0
# proportionally increase the number of filters within each layer. 1.0 is the default number of layers
# used within the paper.
# NB: This option is ignored for any non-mobilenet encoders.
# NB: If loading ImageNet weights, then for MobilenetV1 only values of '0.25', '0.5', '0.75' or '1.0
# can be selected. For MobilenetV2 only values of '0.35', '0.50', '0.75', '1.0', '1.3' or '1.4' can be
# selected. For mobilenet_v3 only values of '0.75' or '1.0' can be selected
# 
# Select a decimal number between 0.1 and 2.0
# [Default: 1.0]
mobilenet_width = 1.0

# The depth multiplier for MobilenetV1 encoder. This is the depth multiplier for depthwise convolution
# (known as the resolution multiplier within the original paper).
# NB: This option is only used for MobilenetV1 and is ignored for all other encoders.
# NB: If loading ImageNet weights, this must be set to 1.
# 
# Select an integer between 1 and 10
# [Default: 1]
mobilenet_depth = 1

# The dropout rate for MobilenetV1 encoder.
# NB: This option is only used for MobilenetV1 and is ignored for all other encoders.
# 
# Select a decimal number between 0.001 and 2.0
# [Default: 0.001]
mobilenet_dropout = 0.001

# Use a minimilist version of MobilenetV3.
# In addition to large and small models MobilenetV3 also contains so-called minimalistic models, these
# models have the same per-layer dimensions characteristic as MobilenetV3 however, they don't utilize
# any of the advanced blocks (squeeze-and-excite units, hard-swish, and 5x5 convolutions). While these
# models are less efficient on CPU, they are much more performant on GPU/DSP.
# NB: This option is only used for MobilenetV3 and is ignored for all other encoders.
# 
# 
# Choose from: True, False
# [Default: False]
mobilenet_minimalistic = False

[model.unbalanced]
# AN UNBALANCED MODEL WITH ADJUSTABLE INPUT SIZE OPTIONS.
# THIS IS AN UNBALANCED MODEL SO B>A SWAPS MAY NOT WORK WELL
# 

# Resolution (in pixels) of the image to train on.
# BE AWARE Larger resolution will dramatically increaseVRAM requirements.
# Make sure your resolution is divisible by 64 (e.g. 64, 128, 256 etc.).
# NB: Your faceset must be at least 1.6x larger than your required input size.
# (e.g. 160 is the maximum input size for a 256x256 faceset).
# 
# Select an integer between 64 and 512
# [Default: 128]
input_size = 128

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# NB: lowmem will override cutom nodes and complexity settings.
# 
# Choose from: True, False
# [Default: False]
lowmem = False

# Number of nodes for decoder. Don't change this unless you know what you are doing!
# 
# Select an integer between 512 and 4096
# [Default: 1024]
nodes = 1024

# Encoder Convolution Layer Complexity. sensible ranges: 128 to 160.
# 
# Select an integer between 64 and 1024
# [Default: 128]
complexity_encoder = 128

# Decoder A Complexity.
# 
# Select an integer between 64 and 1024
# [Default: 384]
complexity_decoder_a = 384

# Decoder B Complexity.
# 
# Select an integer between 64 and 1024
# [Default: 512]
complexity_decoder_b = 512

[model.dfaker]
# DFAKER MODEL (ADAPTED FROM HTTPS://GITHUB.COM/DFAKER/DF)

# Resolution (in pixels) of the output image to generate on.
# BE AWARE Larger resolution will dramatically increase VRAM requirements.
# Must be 128 or 256.
# 
# Select an integer between 128 and 256
# [Default: 128]
output_size = 128

[model.dfl_h128]
# DFL H128 MODEL (ADAPTED FROM HTTPS://GITHUB.COM/IPEROV/DEEPFACELAB)

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# 
# Choose from: True, False
# [Default: False]
lowmem = False

[model.villain]
# A HIGHER RESOLUTION VERSION OF THE ORIGINAL MODEL BY VILLAINGUY.
# EXTREMELY VRAM HEAVY. DON'T TRY TO RUN THIS IF YOU HAVE A SMALL GPU.
# 

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# 
# Choose from: True, False
# [Default: False]
lowmem = False

[model.original]
# ORIGINAL FACESWAP MODEL.

# Lower memory mode. Set to 'True' if having issues with VRAM useage.
# NB: Models with a changed lowmem mode are not compatible with each other.
# 
# Choose from: True, False
# [Default: False]
lowmem = False

[model.realface]
# AN EXTRA DETAILED VARIANT OF ORIGINAL MODEL.
# INCORPORATES IDEAS FROM BRYANLYON AND INSPIRATION FROM THE VILLAIN MODEL.
# REQUIRES ABOUT 6GB-8GB OF VRAM (BATCHSIZE 8-16).
# 

# Resolution (in pixels) of the input image to train on.
# BE AWARE Larger resolution will dramatically increase VRAM requirements.
# Higher resolutions may increase prediction accuracy, but does not effect the resulting output size.
# Must be between 64 and 128 and be divisible by 16.
# 
# Select an integer between 64 and 128
# [Default: 64]
input_size = 64

# Output image resolution (in pixels).
# Be aware that larger resolution will increase VRAM requirements.
# NB: Must be between 64 and 256 and be divisible by 16.
# 
# Select an integer between 64 and 256
# [Default: 128]
output_size = 128

# Number of nodes for decoder. Might affect your model's ability to learn in general.
# Note that: Lower values will affect the ability to predict details.
# 
# Select an integer between 768 and 2048
# [Default: 1536]
dense_nodes = 1536

# Encoder Convolution Layer Complexity. sensible ranges: 128 to 150.
# 
# Select an integer between 96 and 160
# [Default: 128]
complexity_encoder = 128

# Decoder Complexity.
# 
# Select an integer between 512 and 544
# [Default: 512]
complexity_decoder = 512

[trainer.original]
# ORIGINAL TRAINER OPTIONS.
# WARNING: THE DEFAULTS FOR AUGMENTATION WILL BE FINE FOR 99.9% OF USE CASES. ONLY CHANGE THEM IF YOU
# ABSOLUTELY KNOW WHAT YOU ARE DOING!

# Number of sample faces to display for each side in the preview when training.
# 
# Select an integer between 2 and 16
# [Default: 14]
preview_images = 14

# The opacity of the mask overlay in the training preview. Lower values are more transparent.
# 
# Select an integer between 0 and 100
# [Default: 30]
mask_opacity = 30

# The RGB hex color to use for the mask overlay in the training preview.
# 
# [Default: #ff0000]
mask_color = #ff0000

# Percentage amount to randomly zoom each training image in and out.
# 
# Select an integer between 0 and 25
# [Default: 5]
zoom_amount = 5

# Percentage amount to randomly rotate each training image.
# 
# Select an integer between 0 and 25
# [Default: 10]
rotation_range = 10

# Percentage amount to randomly shift each training image horizontally and vertically.
# 
# Select an integer between 0 and 25
# [Default: 5]
shift_range = 5

# Percentage chance to randomly flip each training image horizontally.
# NB: This is ignored if the 'no-flip' option is enabled
# 
# Select an integer between 0 and 75
# [Default: 50]
flip_chance = 50

# Percentage amount to randomly alter the lightness of each training image.
# NB: This is ignored if the 'no-augment-color' option is enabled
# 
# Select an integer between 0 and 75
# [Default: 30]
color_lightness = 30

# Percentage amount to randomly alter the 'a' and 'b' colors of the L*a*b* color space of each
# training image.
# NB: This is ignored if the 'no-augment-color' optionis enabled
# 
# Select an integer between 0 and 50
# [Default: 8]
color_ab = 8

# Percentage chance to perform Contrast Limited Adaptive Histogram Equalization on each training
# image.
# NB: This is ignored if the 'no-augment-color' option is enabled
# 
# This option can be updated for existing models.
# 
# Select an integer between 0 and 75
# [Default: 50]
color_clahe_chance = 50

# The grid size dictates how much Contrast Limited Adaptive Histogram Equalization is performed on any
# training image selected for clahe. Contrast will be applied randomly with a gridsize of 0 up to the
# maximum. This value is a multiplier calculated from the training image size.
# NB: This is ignored if the 'no-augment-color' option is enabled
# 
# Select an integer between 1 and 8
# [Default: 4]
color_clahe_max_size = 4

