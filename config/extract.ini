[global]
# OPTIONS THAT APPLY TO ALL EXTRACTION PLUGINS

# Enable the Tensorflow GPU `allow_growth` configuration option. This option prevents Tensorflow from
# allocating all of the GPU VRAM at launch but can lead to higher VRAM fragmentation and slower
# performance. Should only be enabled if you are having problems running extraction.
# 
# Choose from: True, False
# [Default: False]
allow_growth = False

# Filters out faces below this size. This is a multiplier of the minimum dimension of the frame (i.e.
# 1280x720 = 720). If the original face extract box is smaller than the minimum dimension times this
# multiplier, it is considered a false positive and discarded. Faces which are found to be unusually
# smaller than the frame tend to be misaligned images, except in extreme long-shots. These can be
# usually be safely discarded.
# 
# Select a decimal number between 0.0 and 1.0
# [Default: 0.07]
aligner_min_scale = 0.07

# Filters out faces above this size. This is a multiplier of the minimum dimension of the frame (i.e.
# 1280x720 = 720). If the original face extract box is larger than the minimum dimension times this
# multiplier, it is considered a false positive and discarded. Faces which are found to be unusually
# larger than the frame tend to be misaligned images except in extreme close-ups. These can be usually
# be safely discarded.
# 
# Select a decimal number between 0.0 and 10.0
# [Default: 2.0]
aligner_max_scale = 2.0

# Filters out faces who's landmarks are above this distance from an 'average' face. Values above 15
# tend to be fairly safe. Values above 10 will remove more false positives, but may also filter out
# some faces at extreme angles.
# 
# Select a decimal number between 0.0 and 45.0
# [Default: 22.5]
aligner_distance = 22.5

# Filters out faces who's calculated roll is greater than zero +/- this value in degrees. Aligned
# faces should have a roll value close to zero. Values that are a significant distance from 0 degrees
# tend to be misaligned images. These can usually be safely disgarded.
# 
# Select a decimal number between 0.0 and 90.0
# [Default: 45.0]
aligner_roll = 45.0

# Filters out faces where the lowest point of the aligned face's eye or eyebrow is lower than the
# highest point of the aligned face's mouth. Any faces where this occurs are misaligned and can be
# safely disgarded.
# 
# Choose from: True, False
# [Default: True]
aligner_features = True

# If enabled, and 're-feed' has been selected for extraction, then interim alignments will be filtered
# prior to averaging the final landmarks. This can help improve the final alignments by removing any
# obvious misaligns from the interim results, and may also help pick up difficult alignments. If
# disabled, then all re-feed results will be averaged.
# 
# Choose from: True, False
# [Default: True]
filter_refeed = True

# If enabled, saves any filtered out images into a sub-folder during the extraction process. If
# disabled, filtered faces are deleted. Note: The faces will always be filtered out of the alignments
# file, regardless of whether you keep the faces or not.
# 
# Choose from: True, False
# [Default: False]
save_filtered = False

# If enabled, and 're-align' has been selected for extraction, then all re-feed iterations are re-
# aligned. If disabled, then only the final averaged output from re-feed will be re-aligned.
# 
# Choose from: True, False
# [Default: True]
realign_refeeds = True

# If enabled, and 're-align' has been selected for extraction, then any alignments which would be
# filtered out will not be re-aligned.
# 
# Choose from: True, False
# [Default: True]
filter_realign = True

[mask.vgg_obstructed]
# VGG_OBSTRUCTED OPTIONS. MASK DESIGNED TO PROVIDE SMART SEGMENTATION OF MOSTLY FRONTAL FACES.
# THE MASK MODEL HAS BEEN SPECIFICALLY TRAINED TO RECOGNIZE SOME FACIAL OBSTRUCTIONS (HANDS AND
# EYEGLASSES). PROFILE FACES MAY RESULT IN SUB-PAR PERFORMANCE.

# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too
# high can harm performance.
# 
#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will
# 		automatically be lowered.
# 
# Select an integer between 1 and 64
# [Default: 2]
batch-size = 2

[mask.unet_dfl]
# UNET_DFL OPTIONS. MASK DESIGNED TO PROVIDE SMART SEGMENTATION OF MOSTLY FRONTAL FACES.
# THE MASK MODEL HAS BEEN TRAINED BY COMMUNITY MEMBERS. INSERT MORE COMMENTARY ON TESTING HERE.
# PROFILE FACES MAY RESULT IN SUB-PAR PERFORMANCE.

# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too
# high can harm performance.
# 
#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will
# 		automatically be lowered.
# 
# Select an integer between 1 and 64
# [Default: 8]
batch-size = 8

[mask.vgg_clear]
# VGG_CLEAR OPTIONS. MASK DESIGNED TO PROVIDE SMART SEGMENTATION OF MOSTLY FRONTAL FACES CLEAR OF
# OBSTRUCTIONS.
# PROFILE FACES AND OBSTRUCTIONS MAY RESULT IN SUB-PAR PERFORMANCE.

# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too
# high can harm performance.
# 
#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will
# 		automatically be lowered.
# 
# Select an integer between 1 and 64
# [Default: 6]
batch-size = 6

[mask.bisenet_fp]
# BISENET FACE PARSING OPTIONS.
# MASK PORTED FROM HTTPS://GITHUB.COM/ZLLRUNNING/FACE-PARSING.PYTORCH.

# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too
# high can harm performance.
# 
#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will
# 		automatically be lowered.
# 
# Select an integer between 1 and 64
# [Default: 8]
batch-size = 8

# BiseNet mask still runs fairly quickly on CPU on some setups. Enable CPU mode here to use the CPU
# for this masker to save some VRAM at a speed cost.
# 
# Choose from: True, False
# [Default: False]
cpu = False

# The trained weights to use.
# 
#     - faceswap - Weights trained on wildly varied Faceswap extracted data to better handle varying
# 		conditions, obstructions, glasses and multiple targets within a single extracted image.
#     - original - The original weights trained on the CelebAMask-HQ dataset.
# 
# Choose from: ['faceswap', 'original']
# [Default: faceswap]
weights = faceswap

# Whether to include ears within the face mask.
# 
# Choose from: True, False
# [Default: False]
include_ears = False

# Whether to include hair within the face mask.
# 
# Choose from: True, False
# [Default: False]
include_hair = False

# Whether to include glasses within the face mask.
#     - For 'original' weights excluding glasses will mask out the lenses as well as the frames.
#     - For 'faceswap' weights, the model has been trained to mask out lenses if eyes cannot be seen
# 		(i.e. dark sunglasses) or just the frames if the eyes can be seen.
# 
# Choose from: True, False
# [Default: True]
include_glasses = True

[mask.custom]
# CUSTOM (DUMMY) MASK OPTIONS..
# THE CUSTOM MASK JUST FILLS A FACE PATCH WITH ALL 0'S (MASKED OUT) OR ALL 1'S (MASKED IN) FOR LATER
# MANUAL EDITING. IT DOES NOT USE THE GPU FOR CREATION.

# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too
# high can harm performance.
# 
# Select an integer between 1 and 64
# [Default: 8]
batch-size = 8

# Whether to create a dummy mask with face or head centering.
# 
# Choose from: ['face', 'head']
# [Default: face]
centering = face

# Whether the mask should be filled (True) in which case the custom mask will be created with the
# whole area masked in (i.e. you would need to manually edit out the background) or unfilled (False)
# in which case you would need to manually edit in the face.
# 
# Choose from: True, False
# [Default: False]
fill = False

[recognition.vgg_face2]
# VGG FACE 2 IDENTITY RECOGNITION.
# A KERAS PORT OF THE MODEL TRAINED FOR VGGFACE2: A DATASET FOR RECOGNISING FACES ACROSS POSE AND AGE.
# (HTTPS://ARXIV.ORG/ABS/1710.08092)

# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too
# high can harm performance.
# 
#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will
# 		automatically be lowered.
# 
# Select an integer between 1 and 64
# [Default: 16]
batch-size = 16

# VGG Face2 still runs fairly quickly on CPU on some setups. Enable CPU mode here to use the CPU for
# this plugin to save some VRAM at a speed cost.
# 
# Choose from: True, False
# [Default: False]
cpu = False

[detect.mtcnn]
# MTCNN DETECTOR OPTIONS.
# FAST ON GPU, SLOW ON CPU. USES FEWER RESOURCES THAN OTHER GPU DETECTORS BUT CAN OFTEN RETURN MORE
# FALSE POSITIVES.

# The minimum size of a face (in pixels) to be accepted as a positive match.
# Lower values use significantly more VRAM and will detect more false positives.
# 
# Select an integer between 20 and 1000
# [Default: 20]
minsize = 20

# The scale factor for the image pyramid.
# 
# Select a decimal number between 0.1 and 0.9
# [Default: 0.709]
scalefactor = 0.709

# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too
# high can harm performance.
# 
#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will
# 		automatically be lowered.
# 
# Select an integer between 1 and 64
# [Default: 8]
batch-size = 8

# MTCNN detector still runs fairly quickly on CPU on some setups. Enable CPU mode here to use the CPU
# for this detector to save some VRAM at a speed cost.
# 
# Choose from: True, False
# [Default: True]
cpu = True

# First stage threshold for face detection. This stage obtains face candidates.
# 
# Select a decimal number between 0.1 and 0.9
# [Default: 0.6]
threshold_1 = 0.6

# Second stage threshold for face detection. This stage refines face candidates.
# 
# Select a decimal number between 0.1 and 0.9
# [Default: 0.7]
threshold_2 = 0.7

# Third stage threshold for face detection. This stage further refines face candidates.
# 
# Select a decimal number between 0.1 and 0.9
# [Default: 0.7]
threshold_3 = 0.7

[detect.cv2_dnn]
# CV2 DNN DETECTOR OPTIONS.
# A CPU ONLY EXTRACTOR, IS THE LEAST RELIABLE, BUT USES LEAST RESOURCES AND RUNS FAST ON CPU. USE THIS
# IF NOT USING A GPU AND TIME IS IMPORTANT

# The confidence level at which the detector has succesfully found a face.
# Higher levels will be more discriminating, lower levels will have more false positives.
# 
# Select an integer between 25 and 100
# [Default: 50]
confidence = 50

[detect.s3fd]
# S3FD DETECTOR OPTIONS.
# FAST ON GPU, SLOW ON CPU. CAN DETECT MORE FACES AND FEWER FALSE POSITIVES THAN OTHER GPU DETECTORS,
# BUT IS A LOT MORE RESOURCE INTENSIVE.

# The confidence level at which the detector has succesfully found a face.
# Higher levels will be more discriminating, lower levels will have more false positives.
# 
# Select an integer between 25 and 100
# [Default: 70]
confidence = 70

# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too
# high can harm performance.
# 
#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will
# 		automatically be lowered.
#     - AMD users: A batchsize of 8 requires about 2 GB vram.
# 
# Select an integer between 1 and 64
# [Default: 4]
batch-size = 4

[detect.external]
# IMPORT DETECTOR OPTIONS.
# IMPORTS A DETECTED FACE BOUNDING BOX FROM AN EXTERNAL .JSON FILE.
# 

# The import file should be stored in the same folder as the video (if extracting from a video file)
# or inside the folder of images (if importing from a folder of images)
# 
# [Default: import.json]
file_name = import.json

# The origin (0, 0) location of the co-ordinates system used.
#     - top-left: The origin (0, 0) of the canvas is at the top left corner.
#     - bottom-left: The origin (0, 0) of the canvas is at the bottom left corner.
#     - top-right: The origin (0, 0) of the canvas is at the top right corner.
#     - bottom-right: The origin (0, 0) of the canvas is at the bottom right corner.
# 
# Choose from: ['top-left', 'bottom-left', 'top-right', 'bottom-right']
# [Default: top-left]
origin = top-left

[align.external]
# IMPORT ALIGNER OPTIONS.
# IMPORTS EITHER 68 POINT 2D LANDMARKS OR AN ALIGNED BOUNDING BOX FROM AN EXTERNAL .JSON FILE.

# The import file should be stored in the same folder as the video (if extracting from a video file)
# or inside the folder of images (if importing from a folder of images)
# 
# [Default: import.json]
file_name = import.json

# The origin (0, 0) location of the co-ordinates system used.
#     - top-left: The origin (0, 0) of the canvas is at the top left corner.
#     - bottom-left: The origin (0, 0) of the canvas is at the bottom left corner.
#     - top-right: The origin (0, 0) of the canvas is at the top right corner.
#     - bottom-right: The origin (0, 0) of the canvas is at the bottom right corner.
# 
# Choose from: ['top-left', 'bottom-left', 'top-right', 'bottom-right']
# [Default: top-left]
origin = top-left

# 4 point ROI landmarks only. The approximate centering for the location of the corner points to be
# imported. Default faceswap extracts are generated at 'head' centering, but it is possible to pass in
# ROI points at a tighter centering. Refer to https://github.com/deepfakes/faceswap/pull/1095 for a
# visual guide
#     - head: The ROI points represent a loose crop enclosing the whole head.
#     - face: The ROI points represent a medium crop enclosing the face.
#     - legacy: The ROI points represent a tight crop enclosing the central face area.
#     - none: Only required if importing 4 point ROI landmarks back into faceswap having generated
# 		them from the 'alignments' tool 'export' job.
# 
# Choose from: ['head', 'face', 'legacy', 'none']
# [Default: head]
4_point_centering = head

[align.fan]
# FAN ALIGNER OPTIONS.
# FAST ON GPU, SLOW ON CPU. BEST ALIGNER.

# The batch size to use. To a point, higher batch sizes equal better performance, but setting it too
# high can harm performance.
# 
#     - Nvidia users: If the batchsize is set higher than the your GPU can accomodate then this will
# 		automatically be lowered.
#     - AMD users: A batchsize of 8 requires about 4 GB vram.
# 
# Select an integer between 1 and 64
# [Default: 12]
batch-size = 12

